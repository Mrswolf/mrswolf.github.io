
<!DOCTYPE html>
<html lang="zh-cn">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="swolf的博客">
    <title>主成分分析（Principle Component Analysis) - swolf的博客</title>
    <meta name="author" content="swolf">
    
        <meta name="keywords" content="脑机接口,神经科学,机器学习,swolf,计算机,脑科学,Python,深度学习,PCA,sign ambiguity,sklearn,主成分分析">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"swolf","sameAs":["/about","https://github.com/Mrswolf"],"image":"avatar0.jpg"},"articleBody":"\nPCA是非常常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。\nPCA基本算法PCA的优化问题如下：\n\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\n\\begin{equation*}\n\\begin{aligned}\n& \\argmin_W\n& & -\\mathrm{trace}(W^TXX^TW) \\\\\n& \\text{s.t}\n& & W^TW=I \\\\\n& & & X \\in \\mathbb{R}^{M \\times N} \\\\\n& & & M \\in \\mathbb{R}^{M \\times M}\n\\end{aligned}\n\\end{equation*}其中$X$是数据，$W$是投影矩阵，$N$是样本点个数，$M$是特征个数。PCA要求数据$X$做零均值处理，其解为特征值分解问题：\n\n\\begin{equation*}\n\\begin{aligned}\n(XX^T)w_k = \\lambda w_k\n\\end{aligned}\n\\end{equation*}其中$w_k$是$W$的第k个列向量。PCA后的新数据特征为\n\n\\begin{equation*}\n\\begin{aligned}\n\\hat{X}  = W^T X\n\\end{aligned}\n\\end{equation*}sklearn实现分析svd替代eigsklearn中的PCA实现并未使用eig而是使用svd，主要原因是svd比eig具有更好的数值稳定性（当然代价是其计算时间要比eig更长）。使用svd代替eig也是很多学者如Andrew Ng建议的策略，在StackExchange上也有关于svd和eig的相关讨论讨论1、讨论2。sklearn中直接对数据矩阵$X$而不是协方差矩阵$XX^T$做svd，其等价关系如下：\n\n\\begin{equation*}\n\\begin{aligned}\nX &= U \\Sigma V^T  \\\\\nXX^T &= U \\Sigma^2 U^T \\\\\nW &= U \\\\\n\\hat{X} &= W^TX = \\Sigma V^T\n\\end{aligned}\n\\end{equation*}sign ambiguity问题sklearn的PCA代码中还考虑了svd的sign ambiguity问题，即每个奇异向量的符号在求解过程中是不确定的（例如，将$u_k$和$v_k$同时乘以-1也满足求解条件）。svd算法(包括eig)中的奇异向量符号只是确保数值稳定性的副产品，类似随机分配符号，并无实际意义。\nsklearn使用svd_flip(u, v, u_based_descision=True)函数来确保输出确定性的奇异向量符号，例如，如果u_based_decision=True，则要求$U$的每一列奇异向量中绝对值最大的元素的符号始终为正，$V$也要相对的做出调整。\n鉴于MATLAB是算法开发的标准之一，我很好奇MATLAB是如何处理SVD的sign ambiguity问题的。MATLAB的svd函数的官方文档中有这样一句话:\n\nDifferent machines and releases of MATLAB® can produce different singular vectors that are still numerically accurate. Corresponding columns in U and V can flip their signs, since this does not affect the value of the expression A = USV’.\n\nMATLAB的eig函数的官方文档中亦提到：\n\nFor real eigenvectors, the sign of the eigenvectors can change.\n\n可以看出MATLAB也未保证符号的确定性。同样在MATALB的社区里也有人问了这个问题，并引导我看了这篇Resolving the Sign Ambiguity in the Singular Value Decompostion的文献。\n文献中指出，sklearn的svd_flip方法是一种临时方案（ad hoc），并未从数据分析或者解释的角度来解决sign ambiguity问题。解决sign ambiguity的核心是如何为奇异向量选择一个“有意义”的符号。什么叫“有意义”？比方说我们要研究4种品牌汽车的每公里耗油量，做了4次抽样，构成数据矩阵\n\n\\begin{equation*}\n\\begin{aligned}\nX = \n\\begin{bmatrix}\n4&22&3&5 \\\\\n1&5&1&1 \\\\\n11&69&10&14 \\\\\n11&69&10&14\n\\end{bmatrix}\n\\end{aligned}\n\\end{equation*}其中每一列是一种品牌汽车的耗油量，每一行为抽样情况，svd分解有$X=U \\Sigma V^T$。我们来看一下numpy.linalg.svd计算得到的$V$的第一个奇异向量：\n\n\\begin{equation*}\n\\begin{aligned}\nv_1 = \n\\begin{bmatrix}\n-0.15 \\\\\n-0.96 \\\\\n-0.14 \\\\\n-0.20 \n\\end{bmatrix}\n\\end{aligned}\n\\end{equation*}$v_1$实际指明了耗油量空间的一个向量，然而我们知道耗油量没有负值（如果有的话人类就拥有无限能源了），一个完全指向负的方向没有任何意义，如果改变$v_1$的符号，变为：\n\n\\begin{equation*}\n\\begin{aligned}\nv_1 = \n\\begin{bmatrix}\n0.15 \\\\\n0.96 \\\\\n0.14 \\\\\n0.20 \n\\end{bmatrix}\n\\end{aligned}\n\\end{equation*}结果就合理多了。\n文献中指出，奇异向量的符号应当与大多数数据样本向量的符号相同，从几何上来看，奇异向量应当指向大多数向量指向的方向。下图是我从文献中截取的，深色蓝线是正确的奇异向量方向，浅色蓝线是数据向量。\n\n翻译成数学语言（我按照自己的理解和习惯转化成优化问题，与文献的原始表述并不一致，有兴趣的读者可以看原始文献)，纠正符号算法的核心是对于每一个奇异向量$u_k$，寻找符号$s_k$优化以下目标函数\n\n\\newcommand{\\argmax}{\\mathop{\\mathrm{argmax}}}\n\n\\begin{equation*}\n\\begin{aligned}\n& \\argmax_{s_k \\in \\{ 1, -1 \\}}\n& & s_k (\\sum_{j=1}^M u_k^Tx_{\\ast j} + \\sum_{i=1}^N x_{i \\ast}v_k)\n\\end{aligned}\n\\end{equation*}根据两项求和项的符号即可决定$s_k$的符号，该算法解决左右奇异向量冲突的情况（例如左奇异向量符号是-1,右奇异向量符号为1）的方法是选择求和绝对值最大的奇异向量的符号作为共有符号。文献中指出，该算法仅在上述求和项不为0的情况下有效（在0附近奇异向量的符号为任意情况），具体算法实现如下：\nimport warningsimport numpy as npdef sign_flip(u, s, vh=None):    \"\"\"Flip signs of SVD or EIG.            \"\"\"    if vh is None:        total_proj = np.sum(u*s, axis=0)        signs = np.sign(total_proj)                random_idx = (signs==0)        if np.any(random_idx):            signs[random_idx] = 1            warnings.warn(\"The magnitude is close to zero, the sign will become arbitrary.\")                    u = u*signs                return u, s    else:        left_proj = np.sum(s[:, np.newaxis]*vh, axis=-1)        right_proj = np.sum(u*s, axis=0)        total_proj = left_proj + right_proj        signs = np.sign(total_proj)                random_idx = (signs==0)        if np.any(random_idx):            signs[random_idx] = 1            warnings.warn(\"The magnitude is close to zero, the sign will become arbitrary.\")        u = u*signs        vh = signs[:, np.newaxis]*vh        return u, s, vh\nIncremental PCAPCA算法要求对数据矩阵$X \\in \\mathbb{R}^{M \\times N}$做SVD。显然，当$N$非常大时，没办法完全导入$X$到内存中（试想一下电商场景下的上百万、上千万条数据）。针对PCA的缺陷，Incremental PCA对minibatch的数据做处理，通过不断迭代实现对全部数据的分析。\n未完待续\n","dateCreated":"2019-12-11T15:41:21+08:00","dateModified":"2019-12-28T11:53:03+08:00","datePublished":"2019-12-11T15:41:21+08:00","description":"\nPCA是非常常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。","headline":"主成分分析（Principle Component Analysis)","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://mrswolf.github.io/zh-cn/2019/12/11/PCA/"},"publisher":{"@type":"Organization","name":"swolf","sameAs":["/about","https://github.com/Mrswolf"],"image":"avatar0.jpg","logo":{"@type":"ImageObject","url":"avatar0.jpg"}},"url":"https://mrswolf.github.io/zh-cn/2019/12/11/PCA/","keywords":"matrix decomposition"}</script>
    <meta name="description" content="PCA是非常常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。">
<meta name="keywords" content="PCA,sign ambiguity,sklearn,主成分分析">
<meta property="og:type" content="blog">
<meta property="og:title" content="主成分分析（Principle Component Analysis)">
<meta property="og:url" content="https://mrswolf.github.io/zh-cn/2019/12/11/PCA/index.html">
<meta property="og:site_name" content="swolf的博客">
<meta property="og:description" content="PCA是非常常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。">
<meta property="og:locale" content="zh-cn">
<meta property="og:image" content="https://mrswolf.github.io/zh-cn/2019/12/11/PCA/figure1.png">
<meta property="og:updated_time" content="2019-12-28T03:53:03.373Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="主成分分析（Principle Component Analysis)">
<meta name="twitter:description" content="PCA是非常常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。">
<meta name="twitter:image" content="https://mrswolf.github.io/zh-cn/2019/12/11/PCA/figure1.png">
    
    
        
    
    
        <meta property="og:image" content="https://mrswolf.github.io/assets/images/avatar0.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-du2xmrqdqrl2ollgeiw050kpl6l4nbyz7bumjuurjgsxyopifvukebxc9lqe.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-129079661-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">swolf的博客</a>
    </div>
    
        
            <a class="header-right-picture " href="#about">
        
        
            <img class="header-picture" src="/assets/images/avatar0.jpg" alt="作者的图片">
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/avatar0.jpg" alt="作者的图片">
                </a>
                <h4 class="sidebar-profile-name">swolf</h4>
                
                    <h5 class="sidebar-profile-bio"><p>生物医学工程在读博士，主要研究方向为脑机接口、机器学习和神经科学理论</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/ " title="首页">
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首页</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-categories" title="分类">
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分类</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-tags" title="标签">
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">标签</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-archives" title="归档">
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">归档</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/about" title="关于我">
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">关于我</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://github.com/Mrswolf" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/atom.xml" title="RSS">
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            主成分分析（Principle Component Analysis)
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2019-12-11T15:41:21+08:00">
	
		    12月 11, 2019
    	
    </time>
    
        <span>发布在 </span>
        
    <a class="category-link" href="/categories/每天学点sklearn/">每天学点sklearn</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="table-of-contents">目录</h1><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PCA"><span class="toc-text">PCA</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#基本算法"><span class="toc-text">基本算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn实现分析"><span class="toc-text">sklearn实现分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#svd替代eig"><span class="toc-text">svd替代eig</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sign-ambiguity问题"><span class="toc-text">sign ambiguity问题</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Incremental-PCA"><span class="toc-text">Incremental PCA</span></a></li></ol>
<p>PCA是非常常用的一种矩阵分解算法，PCA通过旋转原始空间来使得数据在各个正交轴上的投影最大，通过选择前几个正交轴可以实现数据降维的目的。<a id="more"></a></p>
<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><h2 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h2><p>PCA的优化问题如下：</p>
<script type="math/tex; mode=display">
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

\begin{equation*}
\begin{aligned}
& \argmin_W
& & -\mathrm{trace}(W^TXX^TW) \\
& \text{s.t}
& & W^TW=I \\
& & & X \in \mathbb{R}^{M \times N} \\
& & & M \in \mathbb{R}^{M \times M}
\end{aligned}
\end{equation*}</script><p>其中$X$是数据，$W$是投影矩阵，$N$是样本点个数，$M$是特征个数。PCA要求数据$X$做零均值处理，其解为特征值分解问题：</p>
<script type="math/tex; mode=display">
\begin{equation*}
\begin{aligned}
(XX^T)w_k = \lambda w_k
\end{aligned}
\end{equation*}</script><p>其中$w_k$是$W$的第k个列向量。PCA后的新数据特征为</p>
<script type="math/tex; mode=display">
\begin{equation*}
\begin{aligned}
\hat{X}  = W^T X
\end{aligned}
\end{equation*}</script><h2 id="sklearn实现分析"><a href="#sklearn实现分析" class="headerlink" title="sklearn实现分析"></a>sklearn实现分析</h2><h3 id="svd替代eig"><a href="#svd替代eig" class="headerlink" title="svd替代eig"></a>svd替代eig</h3><p>sklearn中的PCA实现并未使用eig而是使用svd，主要原因是svd比eig具有更好的数值稳定性（当然代价是其计算时间要比eig更长）。使用svd代替eig也是很多学者如Andrew Ng建议的策略，在StackExchange上也有关于svd和eig的相关讨论<a href="https://stats.stackexchange.com/questions/314046/why-does-andrew-ng-prefer-to-use-svd-and-not-eig-of-covariance-matrix-to-do-pca" target="_blank" rel="noopener">讨论1</a>、<a href="https://stats.stackexchange.com/questions/79043/why-pca-of-data-by-means-of-svd-of-the-data" target="_blank" rel="noopener">讨论2</a>。sklearn中直接对数据矩阵$X$而不是协方差矩阵$XX^T$做svd，其等价关系如下：</p>
<script type="math/tex; mode=display">
\begin{equation*}
\begin{aligned}
X &= U \Sigma V^T  \\
XX^T &= U \Sigma^2 U^T \\
W &= U \\
\hat{X} &= W^TX = \Sigma V^T
\end{aligned}
\end{equation*}</script><h3 id="sign-ambiguity问题"><a href="#sign-ambiguity问题" class="headerlink" title="sign ambiguity问题"></a>sign ambiguity问题</h3><p>sklearn的PCA代码中还考虑了svd的sign ambiguity问题，即每个奇异向量的符号在求解过程中是不确定的（例如，将$u_k$和$v_k$同时乘以-1也满足求解条件）。svd算法(包括eig)中的奇异向量符号只是确保数值稳定性的副产品，类似随机分配符号，并无实际意义。</p>
<p>sklearn使用<code>svd_flip(u, v, u_based_descision=True)</code>函数来确保输出确定性的奇异向量符号，例如，如果<code>u_based_decision=True</code>，则要求$U$的每一列奇异向量中绝对值最大的元素的符号始终为正，$V$也要相对的做出调整。</p>
<p>鉴于MATLAB是算法开发的标准之一，我很好奇MATLAB是如何处理SVD的sign ambiguity问题的。MATLAB的svd函数的<a href="https://www.mathworks.com/help/matlab/ref/double.svd.html;jsessionid=a0f6c96366744a55e10e9731d76b#bu2_0hq-U" target="_blank" rel="noopener">官方文档</a>中有这样一句话:</p>
<blockquote>
<p>Different machines and releases of MATLAB® can produce different singular vectors that are still numerically accurate. Corresponding columns in U and V can flip their signs, since this does not affect the value of the expression A = U<em>S</em>V’.</p>
</blockquote>
<p>MATLAB的eig函数的<a href="https://www.mathworks.com/help/matlab/ref/eig.html#btgapg5-1-V" target="_blank" rel="noopener">官方文档</a>中亦提到：</p>
<blockquote>
<p>For real eigenvectors, the sign of the eigenvectors can change.</p>
</blockquote>
<p>可以看出MATLAB也未保证符号的确定性。同样在MATALB的社区里也有人问了这个<a href="https://www.mathworks.com/matlabcentral/answers/325234-request-for-guidance-about-the-sign-of-singular-value-decomposition" target="_blank" rel="noopener">问题</a>，并引导我看了这篇<a href="https://www.sandia.gov/~tgkolda/pubs/pubfiles/SAND2007-6422.pdf" target="_blank" rel="noopener">Resolving the Sign Ambiguity in the Singular Value Decompostion</a>的文献。</p>
<p>文献中指出，sklearn的<code>svd_flip</code>方法是一种临时方案（ad hoc），并未从数据分析或者解释的角度来解决sign ambiguity问题。解决sign ambiguity的核心是如何为奇异向量选择一个“有意义”的符号。什么叫“有意义”？比方说我们要研究4种品牌汽车的每公里耗油量，做了4次抽样，构成数据矩阵</p>
<script type="math/tex; mode=display">
\begin{equation*}
\begin{aligned}
X = 
\begin{bmatrix}
4&22&3&5 \\
1&5&1&1 \\
11&69&10&14 \\
11&69&10&14
\end{bmatrix}
\end{aligned}
\end{equation*}</script><p>其中每一列是一种品牌汽车的耗油量，每一行为抽样情况，svd分解有$X=U \Sigma V^T$。我们来看一下<code>numpy.linalg.svd</code>计算得到的$V$的第一个奇异向量：</p>
<script type="math/tex; mode=display">
\begin{equation*}
\begin{aligned}
v_1 = 
\begin{bmatrix}
-0.15 \\
-0.96 \\
-0.14 \\
-0.20 
\end{bmatrix}
\end{aligned}
\end{equation*}</script><p>$v_1$实际指明了耗油量空间的一个向量，然而我们知道耗油量没有负值（如果有的话人类就拥有无限能源了），一个完全指向负的方向没有任何意义，如果改变$v_1$的符号，变为：</p>
<script type="math/tex; mode=display">
\begin{equation*}
\begin{aligned}
v_1 = 
\begin{bmatrix}
0.15 \\
0.96 \\
0.14 \\
0.20 
\end{bmatrix}
\end{aligned}
\end{equation*}</script><p>结果就合理多了。</p>
<p>文献中指出，<strong>奇异向量的符号应当与大多数数据样本向量的符号相同，从几何上来看，奇异向量应当指向大多数向量指向的方向</strong>。下图是我从文献中截取的，深色蓝线是正确的奇异向量方向，浅色蓝线是数据向量。</p>
<img src="/zh-cn/2019/12/11/PCA/figure1.png" title="正确符号示意">
<p>翻译成数学语言（我按照自己的理解和习惯转化成优化问题，与文献的原始表述并不一致，有兴趣的读者可以看原始文献)，纠正符号算法的核心是对于每一个奇异向量$u_k$，寻找符号$s_k$优化以下目标函数</p>
<script type="math/tex; mode=display">
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}

\begin{equation*}
\begin{aligned}
& \argmax_{s_k \in \{ 1, -1 \}}
& & s_k (\sum_{j=1}^M u_k^Tx_{\ast j} + \sum_{i=1}^N x_{i \ast}v_k)
\end{aligned}
\end{equation*}</script><p>根据两项求和项的符号即可决定$s_k$的符号，该算法解决左右奇异向量冲突的情况（例如左奇异向量符号是-1,右奇异向量符号为1）的方法是选择求和绝对值最大的奇异向量的符号作为共有符号。文献中指出，该算法仅在上述求和项不为0的情况下有效（在0附近奇异向量的符号为任意情况），具体算法实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sign_flip</span><span class="params">(u, s, vh=None)</span>:</span></span><br><span class="line">    <span class="string">"""Flip signs of SVD or EIG.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> vh <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        total_proj = np.sum(u*s, axis=<span class="number">0</span>)</span><br><span class="line">        signs = np.sign(total_proj)</span><br><span class="line">        </span><br><span class="line">        random_idx = (signs==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> np.any(random_idx):</span><br><span class="line">            signs[random_idx] = <span class="number">1</span></span><br><span class="line">            warnings.warn(<span class="string">"The magnitude is close to zero, the sign will become arbitrary."</span>)</span><br><span class="line">            </span><br><span class="line">        u = u*signs</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> u, s</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        left_proj = np.sum(s[:, np.newaxis]*vh, axis=<span class="number">-1</span>)</span><br><span class="line">        right_proj = np.sum(u*s, axis=<span class="number">0</span>)</span><br><span class="line">        total_proj = left_proj + right_proj</span><br><span class="line">        signs = np.sign(total_proj)</span><br><span class="line">        </span><br><span class="line">        random_idx = (signs==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> np.any(random_idx):</span><br><span class="line">            signs[random_idx] = <span class="number">1</span></span><br><span class="line">            warnings.warn(<span class="string">"The magnitude is close to zero, the sign will become arbitrary."</span>)</span><br><span class="line"></span><br><span class="line">        u = u*signs</span><br><span class="line">        vh = signs[:, np.newaxis]*vh</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> u, s, vh</span><br></pre></td></tr></table></figure>
<h1 id="Incremental-PCA"><a href="#Incremental-PCA" class="headerlink" title="Incremental PCA"></a>Incremental PCA</h1><p>PCA算法要求对数据矩阵$X \in \mathbb{R}^{M \times N}$做SVD。显然，当$N$非常大时，没办法完全导入$X$到内存中（试想一下电商场景下的上百万、上千万条数据）。针对PCA的缺陷，Incremental PCA对minibatch的数据做处理，通过不断迭代实现对全部数据的分析。</p>
<p><strong>未完待续</strong></p>

            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">标签</span><br>
                
    <a class="tag tag--primary tag--small t-link" href="/tags/matrix-decomposition/">matrix decomposition</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/zh-cn/2019/05/24/manjaro踩坑记/" data-tooltip="manjaro踩坑记" aria-label="下一篇: manjaro踩坑记">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Diesen Beitrag teilen">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2019 swolf. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/zh-cn/2019/05/24/manjaro踩坑记/" data-tooltip="manjaro踩坑记" aria-label="下一篇: manjaro踩坑记">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Diesen Beitrag teilen">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="5">
    <i id="btn-close-shareoptions" class="fa fa-times"></i>
    <ul class="share-options">
        
    </ul>
</div>

            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/avatar0.jpg" alt="作者的图片">
        
            <h4 id="about-card-name">swolf</h4>
        
            <div id="about-card-bio"><p>生物医学工程在读博士，主要研究方向为脑机接口、机器学习和神经科学理论</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br>
                <p>目前失业中…</p>

            </div>
        
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cyberpunk.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-vufjrm3fmbuttogo1hxuu0w9w0sesk5iyysjuguc2hdhufot9szxg8twijry.min.js"></script>
<!--SCRIPTS END-->



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
