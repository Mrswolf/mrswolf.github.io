
<!DOCTYPE html>
<html lang="zh-cn">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="swolf的博客">
    <title>详解CSP（持续更新中） - swolf的博客</title>
    <meta name="author" content="swolf">
    
        <meta name="keywords" content="脑机接口,神经科学,机器学习,swolf,计算机,脑科学,Python,深度学习,bci,common spatial pattern,spatial filter,algorithm">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"swolf","sameAs":["/about","https://github.com/Mrswolf"],"image":"avatar0.jpg"},"articleBody":"\n什么是CSP共空间模式(common spatial pattern,CSP)是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年没有什么较大的突破。尽管如此，CSP中的一些数学思想对传统脑-机接口算法仍然具有较大的影响力，例如近年运用在SSVEP上的TRCA、DCPM等算法均和CSP有着异曲同工之妙。因此，本文从CSP原始算法出发，讨论其变形和一系列改进算法，试图为读者阐明其中的数学思想。\nCSP的历史1970年，Fukunaga和Koontz在IEEE Transactions on Computers上发表论文，介绍了一种特征选择的方法，史称“Fukunaga-Koontz变换”，这种特征选择的方法迅速在各个领域得到推广。\n1990年，Koles等人将“Fukunaga-Koontz变换”引入背景脑电分析，发现可以通过脑电有效的区分健康人群和精神病人群。 \n1999年，MuÈller-Gerking和Pfurtscheller等人（脑-机接口领域有名的Graz研究中心）把Koles的方法（他们对Koles的方法做了一些微小的改进）应用到运动分类上，并称这种方法为“common spatial pattern，CSP”。\n次年，Graz小组的人又把CSP用在运动想象的分类上，取得比较好的分类效果，奠定了CSP在运动想象领域的地位。\nCSP（二分类）2000年Graz的论文中提出的CSP是为二分类问题设计的，形式较为简单，然而如果你读CSP相关论文，就会发现CSP存在至少三种表述形式。这三种方式相互联系，又有所区分，很容易让初学者陷入混乱，不知道哪一种是正确形式。\n为了解决这一问题，使读者更好的理解其中的数学本质，我接下来从2000年Graz论文中的算法出发，讨论三种形式间的联系和不同。当然，我假设读者具有线性代数知识（了解特征值分解，了解一些矩阵的操作方法和符号）以及一些基础的numpy或MATLAB数据处理经验，并请读者牢记以下格言\nDON’T PANIC\nThe Hitchhiker's Guide to the GalaxyDouglas Adams\nCSP的原始形式假设我们做脑电实验，采集到两类不同的任务信号，用矩阵形式可以表示为$E^{(i)}_1 \\in R^{N_c \\times S}$和$E^{(i)}_2 \\in R^{N_c \\times S}$，其中$N_c$表示脑电导联数目，$S$表示采样点的个数，而上标$(i)$表示试次的序号（脑电实验通常会对一种任务进行多次实验，得到很多试次的数据），原始CSP算法采用以下步骤：\n第一步，对数据做decenter处理，即减去每一导联在样本点上的均值\n\nE^{(i)}_1 = E^{(i)}_1 - mean(E^{(i)}_1, axis=1) \\\\\nE^{(i)}_2 = E^{(i)}_2 - mean(E^{(i)}_2, axis=1)第二步，求每一试次的协方差矩阵并归一化，最后得到平均的协方差矩阵\n\n\\bar{C}_1 = \\sum_{i} \\frac{E^{(i)}_1(E^{(i)}_1)^{T}}{trace(E^{(i)}_1(E^{(i)}_1)^{T})} \\\\\n\\bar{C}_2 = \\sum_{i} \\frac{E^{(i)}_2(E^{(i)}_2)^{T}}{trace(E^{(i)}_2(E^{(i)}_2)^{T})}这里的函数$trace()$是求矩阵的迹，即主对角线上的元素之和。注意到对于去均值的矩阵$E$，其协方差矩阵可以表示为$C = \\frac{1}{S-1} EE^{T}$，上式中没有$S-1$出现是因为上下相除互相抵消。$\\bar{C}_1$和$\\bar{C}_2$则是两类任务信号的平均协方差矩阵。\n为什么要使用$trace()$来对协方差矩阵归一化？\n1990年Koles的文章中指出，归一化的目的是为了消除”被试间脑电信号幅值的变化”，注意到Koles的原意是区分健康人群和精神疾病人群，而个体的脑电幅值是有绝对性的差异的。方差可以表征信号在时域上的能量高低，不同人群的协方差矩阵的绝对值不同。为了消除这种差异带来的影响，利用$trace()$函数求得所有导联的总体能量，并对协方差矩阵归一化，从而安排除不同个体带来的干扰。Graz小组对同一个体不同试次的数据沿用了这种归一化方式，试图消除试次间的差异，发现也有一定的作用，这种归一化方式就一直流传下来。\n然而，有些分析显示这种归一化方式反而会削弱信号的可分性，建议不要使用归一化。关于这一点，我会在以后进行讨论。\n\n第三步， 构建复合协方差矩阵，并特征值分解，构建白化（whitening）矩阵\n\nC_c = \\bar{C}_1 + \\bar{C}_2 \\\\\n\nC_c = V_cD_cV_c^{T}\n=\n\\begin{bmatrix}\nv_1^{(c)} &v_2^{(c)} &\\cdots &v_{N_c}^{(c)}\n\\end{bmatrix}\n\n\\begin{bmatrix}\n\\lambda_1^{(c)} & & &\\\\\n& \\lambda_2^{(c)} & &\\\\\n& & \\ddots &\\\\\n& & & \\lambda_{N_c}^{(c)}\n\\end{bmatrix} \n\n\\begin{bmatrix}\nv_1^{(c)} \\\\\nv_2^{(c)} \\\\\n\\vdots \\\\\nv_{N_c}^{(c)}\n\\end{bmatrix}\n\\\\\n\nP = D_c^{-1/2}V_c^{T}其中$V_c$是特征向量矩阵（每一列是特征向量），$D_c$是由特征值组成的对角矩阵。$P$是白化矩阵，其目的是把对角矩阵$D_c$变化为单位矩阵$I$,即$PC_cP^{T} = I$成立。\n第四步，计算空间滤波器$W$\n\n\\begin{align}\nI = PC_cP^{T} &= P\\bar{C}_1P^{T} + P\\bar{C}_2P^{T} \\\\\n&= S_1 + S_2 \\\\\n\\end{align} \\\\\n\nS_1 = VD_1V^{T} \n= \n\\begin{bmatrix}\nv_1 &v_2 &\\cdots &v_{N_c}\n\\end{bmatrix}\n\n\\begin{bmatrix}\n\\lambda_1^{(1)} & & &\\\\\n& \\lambda_2^{(1)} & &\\\\\n& & \\ddots &\\\\\n& & & \\lambda_{N_c}^{(1)} \\\\\n\\end{bmatrix} \n\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\vdots \\\\\nv_{N_c} \\\\\n\\end{bmatrix}\\\\\n\nS_2 = VD_2V^{T} \n= \n\\begin{bmatrix}\nv_1 &v_2 &\\cdots &v_{N_c}\n\\end{bmatrix}\n\n\\begin{bmatrix}\n\\lambda_1^{(2)} & & &\\\\\n& \\lambda_2^{(2)} & &\\\\\n& & \\ddots &\\\\\n& & & \\lambda_{N_c}^{(2)} \\\\\n\\end{bmatrix} \n\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\vdots \\\\\nv_{N_c} \\\\\n\\end{bmatrix}\\\\\n\nW = P^TV其中矩阵$S_1$和$S_2$具有同样的特征向量$V$（这也是共空间模式名称的由来），而相对应的特征值相加始终为1，即$D_1 + D_2 = I$。\n为什么$S_1$和$S_2$具有同样的特征向量和此消彼长的特征值关系？这一点可以简单的证明如下假设$v_j$和$\\lambda_j^{(1)}$分别是$S_1$的特征向量和特征值，即\n\nS_1v_j=\\lambda_j^{(1)}v_j注意到$S_1+S_2=I$，把上式中的$S_1$置换掉可得\n\n(I-S_2)v_j=\\lambda_j^{(1)}v_j把上式变形一下可得\n\nS_2v_j=(1-\\lambda_j^{(1)})v_j显然$v_j$也是$S_2$的特征向量，只不过其特征值为$1-\\lambda_j^{(1)}$\n\n这里还有一点需要注意，我们假设$S_1$特征值的顺序是按降序排列的（那么$S_2$的特征值就是按升序排列），即\n\n1 \\ge \\lambda_1^{(1)} \\ge \\lambda_2^{(1)} \\ge \\dots \\ge \\lambda_{N_c}^{(1)} \\ge 0\\\\\n0 \\le \\lambda_1^{(2)} \\le \\lambda_2^{(2)} \\le \\dots \\le \\lambda_{N_c}^{(2)} \\le 1\\\\这种排序的主要目的是为了以后分析的便利性，例如在运动想象分类中提取最有效的空间滤波器。协方差矩阵是半正定矩阵（positive semidefinite），而半正定矩阵的特征值均为非负。故$S_1$和$S_2$的特征值在0~1之间\n\n以上就是原始CSP算法的基本内容，在得到空间滤波器矩阵$W$后（$W$的每一列都是一个空间滤波器），就可以对信号进行变换$Z=W^TE$，而$Z$的每一行则代表了滤波后的一个时序特征信号，接下来便可以对$Z$做进一步的分析。\n简单回顾一下CSP算法，不难发现CSP实质求解的是这样一个问题，寻找正交矩阵$W$使得以下条件成立：\n\nW^T\\bar{C}_1W= D_1 \\\\\nW^T\\bar{C}_2W= D_2 \\\\\nD_1 + D_2 = I \\\\让我们对以上的公式做一些变换，把第一个和第二个公式相加\n\nW^T(\\bar{C}_1+\\bar{C}_2)W=D_1+D_2=I又因为$W$是正交矩阵，故$W^T=W^{-1}$，从而\n\n(\\bar{C}_1+\\bar{C}_2)W=W把上式代入$\\bar{C}_1W=WD_1$，可得\n\n\\bar{C}_1W=(\\bar{C}_1+\\bar{C}_2)WD_1这个式子是不是看起来很像特征向量定义的公式$\\bar{C}_1W=WD_1$呢？只不过等式右边多了一个矩阵$\\bar{C}_1+\\bar{C}_2$。\n这类形式的问题叫广义特征值问题，求解广义特征值问题是脑-机接口领域传统空间滤波方法的基础，大量的算法（CSP、TRCA等）都可以转化为这一形式。\n下一小节中，我将从$\\bar{C}_1W=(\\bar{C}_1+\\bar{C}_2)WD_1$这一公式出发来探讨CSP的第二种表述形式。\nCSP的第二种表述在讨论CSP的第二种表述之前，我们需要了解一个数学概念广义雷利商（generalized Rayleigh quotient）。\n广义雷利商长这样\n\nR=\\frac{w^TAw}{w^TBw} \\\\\nA, B \\succeq 0 \\\\其中$A$和$B$为半正定矩阵（读者可以简单理解为协方差矩阵），$w$是列向量，显然广义雷利商$R$是一个实数。\n如果我们求如下广义雷利商的优化问题，就会有一些有趣的结果\n\n\\underset{w}{\\mathrm{max}} \\frac{w^TAw}{w^TBw}寻找$w$使得$R$最大，在数学上可以等价为求解下式（我就不证明了，感兴趣的读者可点击广义雷利商的链接查看证明过程）\n\nAw=Bw\\lambda_1这个公式就是上一节提到的广义特征值问题，也就是说，寻找$w$使广义雷利商最大可以等价为求解$A$和$B$的广义特征值问题并找到使特征值$\\lambda_1$最大所对应的特征向量$w$。\n如果我们继续寻找能够使$R$第二大、第三大的$w$，就会发现只要解出广义特征值问题的矩阵形式即可\n\nAW= BWD\n= B \n\\begin{bmatrix}\nw_1 &w_2 &\\cdots &w_N\n\\end{bmatrix}\n\n\\begin{bmatrix}\n\\lambda_1 & & &\\\\\n& \\lambda_2 & &\\\\\n& & \\ddots &\\\\\n& & & \\lambda_N \\\\\n\\end{bmatrix}其中$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_N$, 特征值的数值也就是广义雷利商的数值。\n如果读者明白广义雷利商和广义特征值问题之间的关联，就不难发现，上一节中推导的CSP求解问题可以变形为求解广义雷利商问题\n\n\\bar{C}_1W=(\\bar{C}_1+\\bar{C}_2)WD_1 \\ \\ \\Longleftrightarrow \\ \\ \\underset{w}{\\mathrm{max}} \\frac{w^T\\bar{C}_1w}{w^T(\\bar{C}_1+\\bar{C}_2)w}这里$A=\\bar{C}_1$、$B=\\bar{C}_1+\\bar{C}_2$，$W$矩阵是广义雷利商第一大、第二大至第N大向量$w$组成的集合。\n这里我们推出了CSP问题的第二种表述形式，即\n\n\\underset{w}{\\mathrm{max}} \\frac{w^T\\bar{C}_1w}{w^T(\\bar{C}_1+\\bar{C}_2)w}CSP的第三种表述CSP的第三种表述形式需要绕点弯路。首先还是从CSP的原始形式出发，即寻找正交矩阵$W$使得以下条件成立：\n\nW^T\\bar{C}_1W= D_1 \\\\\nW^T\\bar{C}_2W= D_2 \\\\\nD_1 + D_2 = I \\\\在第二个公式的左右两边同时右乘矩阵$W^{-1}\\bar{C}_2^{-1}$，可以得到\n\nW^T=D_2W^{-1}\\bar{C}_2^{-1}把该式代入$W^T\\bar{C}_1W= D_1$，替换掉$W^T$，可得\n\nD_2W^{-1}\\bar{C}_2^{-1}\\bar{C}_1W= D_1上式左右两边左乘$\\bar{C}_2WD_2^{-1}$，可得\n\n\\begin{align}\n\\bar{C}_1W &= \\bar{C}_2WD_2^{-1}D_1 \\\\\n&=\\bar{C}_2W\\Lambda \\\\\n\\end{align} \\\\\n\\Lambda = D_2^{-1}D_1没错，我们又推出了熟悉的广义特征值问题$\\bar{C}_1W=\\bar{C}_2W\\Lambda$，再考虑广义雷利商与之的联系，可以得到CSP的第三种表述\n\n\\underset{w}{\\mathrm{max}} \\frac{w^T\\bar{C}_1w}{w^T\\bar{C}_2w}相比CSP的原始形式和第二种表述形式，第三种表述形式更适合从直观上解释CSP在运动想象上有效的原因。运动想象会产生事件相关同步（ERS）和事件相关去同步（ERD）的现象，简单来说就是从电信号上看，某些脑区能量升高，某些脑区能量降低，故能量变化才是运动想象分类的关键特征。\n我们前面提高过，方差可以看作一导信号能量的高低（协方差矩阵则是多导信号的能量反应），因此CSP的第三种表述形式实质体现这样一个问题：寻找一种变换方式$w$，使得变换后任务1的能量（$w^T\\bar{C}_1w$）和任务2的能量（$w^T\\bar{C}_2w$）差异最大化（其比值最大）。\nCSP的这种特性恰好和运动想象的现象一致，CSP对能量特征做转换，强化了不同任务间能量差异。\n\n关于CSP的第三种表述，最后还需要注意的一点是其同CSP原始形式和第二种表述形式并不完全等价，我们在推导第三种表述形式过程种始终没有用到这样一个约束条件$D_1 + D_2 = I$。\n这表明，第三种形式是CSP的一种泛化形式，其和CSP原始形式和第二种表述的差异仅在于特征值$\\Lambda$不要求在0~1的范围内，具体来说，它们的特征值间存在这样一种关系\n\n\\Lambda = D_2^{-1}D_1 \\\\\nD_1 = \\Lambda(\\Lambda + I)^{-1} \\\\\nD_2 = (\\Lambda + I)^{-1} \\\\Talk is cheap. Show me the code.TL;DR我的实践经验表明，CSP第二种和第三种形式总要优于原始形式，主要是能够避免很多数值精度产生的问题。\n如果你想试试原生的CSP，选第二种形式。\n如果你能够理解其本质，第三种形式或许更合适(我喜欢第三种形式，因为从中可以引出CSP和Riemannian Geometry的关系，这一点以后再谈)\n以下是三种CSP的代码import numpy as npimport scipy.linalg as linalg# generate dataX1 = np.random.rand(20, 60, 1000)X2 = np.random.rand(20, 60, 1000)def csp1(X1, X2):    '''    The first form of CSP.    X1: # of trials, # of channels, # of samples    X2: # of trials, # of channels, # of samples        Return    W: spatial fitlers    D1: eigenvalues of filters    '''     X1 = X1 - X1.mean(axis=2, keepdims=True)    X2 = X2 - X2.mean(axis=2, keepdims=True)    # normalization covariance    C1 = []    C2 = []    for i in range(X1.shape[0]):        tmp = X1[i,:,:].dot(X1[i,:,:].T)        tmp = tmp/np.trace(tmp)        C1.append(tmp)    for i in range(X2.shape[0]):        tmp = X2[i,:,:].dot(X2[i,:,:].T)        tmp = tmp/np.trace(tmp)        C2.append(tmp)    C1 = np.array(C1)    C2 = np.array(C2)    # average covariance    mean_C1 = C1.mean(axis=0)    mean_C2 = C2.mean(axis=0)    mean_C = mean_C1 + mean_C2    # whitening matrix    D_c, V_c = linalg.eigh(mean_C)    isqrt_D_c = np.diag(np.sqrt(1/D_c))    P = isqrt_D_c@V_c.T    # S1 and S2    S1 = P@mean_C1@P.T    S2 = P@mean_C2@P.T    # spatial filters    D1, V = linalg.eigh(S1)    W = P.T@V    return W, D1def csp2(X1, X2):    '''    The second form of CSP.    X1: # of trials, # of channels, # of samples    X2: # of trials, # of channels, # of samples        Return    W: spatial fitlers    D1: eigenvalues of filters    '''     X1 = X1 - X1.mean(axis=2, keepdims=True)    X2 = X2 - X2.mean(axis=2, keepdims=True)    # normalization covariance    C1 = []    C2 = []    for i in range(X1.shape[0]):        tmp = X1[i,:,:].dot(X1[i,:,:].T)        tmp = tmp/np.trace(tmp)        C1.append(tmp)    for i in range(X2.shape[0]):        tmp = X2[i,:,:].dot(X2[i,:,:].T)        tmp = tmp/np.trace(tmp)        C2.append(tmp)    C1 = np.array(C1)    C2 = np.array(C2)    # average covariance    mean_C1 = C1.mean(axis=0)    mean_C2 = C2.mean(axis=0)    mean_C = mean_C1 + mean_C2    # generalized eigenvalue problem    D1, W = linalg.eigh(mean_C1,mean_C)    return W, D1def csp3(X1, X2):    '''    The third form of CSP    X1: # of trials, # of channels, # of samples    X2: # of trials, # of channels, # of samples        Return    W: spatial fitlers    D: eigenvalues of filters    '''     X1 = X1 - X1.mean(axis=2, keepdims=True)    X2 = X2 - X2.mean(axis=2, keepdims=True)    # normalization covariance    C1 = []    C2 = []    for i in range(X1.shape[0]):        tmp = X1[i,:,:].dot(X1[i,:,:].T)        tmp = tmp/np.trace(tmp)        C1.append(tmp)    for i in range(X2.shape[0]):        tmp = X2[i,:,:].dot(X2[i,:,:].T)        tmp = tmp/np.trace(tmp)        C2.append(tmp)    C1 = np.array(C1)    C2 = np.array(C2)    # average covariance    mean_C1 = C1.mean(axis=0)    mean_C2 = C2.mean(axis=0)    D, W = linalg.eigh(mean_C1, mean_C2)    return W, D\nW1, D1 = csp1(X1, X2)W2, D2 = csp2(X1, X2)W3, D3 = csp3(X1, X2)\n让我们看一下CSP原始形式的第一个空间滤波器的特征值\nD1[0]\n0.4601822497625514\n再看一下CSP第二种表述形式的第一个空间滤波器的特征值，两者大致相同，但最后几位不太一样，这是由于数值舍入精度的问题（或许跟如何实施算法有关），关于这方面的问题我也在研究中，目前就忽略吧。\nD2[0]\n0.46018224976254957\n再看一下CSP第三种表述形式的第一个空间滤波器的特征值，好像跟上面两者差别很大呢。\nD3[0]\n0.8524770620457159\n回顾一下前文讨论的特征值间的联系，简单的做个变换，发现我们的算法是正确的。\n# connection between form2 and form3new_D = D3/(D3+1)new_D[0]\n0.4601822497625497\n最后来看下空间滤波器（特征向量）是否正确，我们来检验第一特征向量的前5个数值。\nW1[:5, 0]\narray([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])\n第二种形式和CSP原始形式完全一致（有时会差一个正负号，但从算法上来说是完全正确的）\n# maybe negative, but that's okW2[:5, 0]\narray([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])\n看一下第三种形式。咦，好像不太对哦。\n# doesn't look like the result above. Is there something wrong?W3[:5, 0]\narray([ 2.13472472,  0.9693093 , -0.69313345,  0.71057786, -0.40544634])\n别慌，第三种形式只和第一种和第二种差了一个倍数（向量是一样的），做如下变换就能得到一样的结果。\n# Nothing is wrong here.np.sqrt(1/(D3+1))[0]*W3[:5, 0]\narray([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])\nCSP（多分类）","dateCreated":"2019-01-06T15:41:21+08:00","dateModified":"2019-05-20T14:45:52+08:00","datePublished":"2019-01-06T15:41:21+08:00","description":"\n什么是CSP共空间模式(common spatial pattern,CSP)是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年没有什么较大的突破。尽管如此，CSP中的一些数学思想对传统脑-机接口算法仍然具有较大的影响力，例如近年运用在SSVEP上的TRCA、DCPM等算法均和CSP有着异曲同工之妙。因此，本文从CSP原始算法出发，讨论其变形和一系列改进算法，试图为读者阐明其中的数学思想。","headline":"详解CSP（持续更新中）","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/"},"publisher":{"@type":"Organization","name":"swolf","sameAs":["/about","https://github.com/Mrswolf"],"image":"avatar0.jpg","logo":{"@type":"ImageObject","url":"avatar0.jpg"}},"url":"https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/","keywords":"brain-computer interface"}</script>
    <meta name="description" content="什么是CSP共空间模式(common spatial pattern,CSP)是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年没有什么较大的突破。尽管如此，CSP中的一些数学思想对传统脑-机接口算法仍然具有较大的影响力，例如近年运用在SSVEP上的TRCA、DCPM">
<meta name="keywords" content="bci,common spatial pattern,spatial filter,algorithm">
<meta property="og:type" content="blog">
<meta property="og:title" content="详解CSP（持续更新中）">
<meta property="og:url" content="https://mrswolf.github.io/zh-cn/2019/01/06/详解CSP/index.html">
<meta property="og:site_name" content="swolf的博客">
<meta property="og:description" content="什么是CSP共空间模式(common spatial pattern,CSP)是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年没有什么较大的突破。尽管如此，CSP中的一些数学思想对传统脑-机接口算法仍然具有较大的影响力，例如近年运用在SSVEP上的TRCA、DCPM">
<meta property="og:locale" content="zh-cn">
<meta property="og:updated_time" content="2019-05-20T06:45:52.354Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="详解CSP（持续更新中）">
<meta name="twitter:description" content="什么是CSP共空间模式(common spatial pattern,CSP)是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年没有什么较大的突破。尽管如此，CSP中的一些数学思想对传统脑-机接口算法仍然具有较大的影响力，例如近年运用在SSVEP上的TRCA、DCPM">
    
    
        
    
    
        <meta property="og:image" content="https://mrswolf.github.io/assets/images/avatar0.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-du2xmrqdqrl2ollgeiw050kpl6l4nbyz7bumjuurjgsxyopifvukebxc9lqe.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-129079661-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">swolf的博客</a>
    </div>
    
        
            <a class="header-right-picture " href="#about">
        
        
            <img class="header-picture" src="/assets/images/avatar0.jpg" alt="作者的图片">
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/avatar0.jpg" alt="作者的图片">
                </a>
                <h4 class="sidebar-profile-name">swolf</h4>
                
                    <h5 class="sidebar-profile-bio"><p>生物医学工程在读博士，主要研究方向为脑机接口、机器学习和神经科学理论</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/ " title="首页">
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首页</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-categories" title="分类">
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分类</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-tags" title="标签">
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">标签</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-archives" title="归档">
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">归档</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/about" title="关于我">
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">关于我</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://github.com/Mrswolf" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/atom.xml" title="RSS">
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            详解CSP（持续更新中）
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2019-01-06T15:41:21+08:00">
	
		    1月 06, 2019
    	
    </time>
    
        <span>发布在 </span>
        
    <a class="category-link" href="/categories/脑-机接口算法大全/">脑-机接口算法大全</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <h1 id="table-of-contents">目录</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是CSP"><span class="toc-text">什么是CSP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CSP的历史"><span class="toc-text">CSP的历史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CSP（二分类）"><span class="toc-text">CSP（二分类）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CSP的原始形式"><span class="toc-text">CSP的原始形式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CSP的第二种表述"><span class="toc-text">CSP的第二种表述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CSP的第三种表述"><span class="toc-text">CSP的第三种表述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Talk-is-cheap-Show-me-the-code"><span class="toc-text">Talk is cheap. Show me the code.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CSP（多分类）"><span class="toc-text">CSP（多分类）</span></a></li></ol></li></ol>
<h2 id="什么是CSP"><a href="#什么是CSP" class="headerlink" title="什么是CSP"></a>什么是CSP</h2><p>共空间模式(common spatial pattern,CSP)是脑-机接口领域常用的一类空间滤波算法，尤其在运动想象范式分类上具有较好的效果，是运动想象范式的基准算法之一。目前，CSP及其改进算法的发展速度放缓，看似到达了算法的瓶颈期，近几年没有什么较大的突破。尽管如此，CSP中的一些数学思想对传统脑-机接口算法仍然具有较大的影响力，例如近年运用在SSVEP上的TRCA、DCPM等算法均和CSP有着异曲同工之妙。因此，本文从CSP原始算法出发，讨论其变形和一系列改进算法，试图为读者阐明其中的数学思想。<br><a id="more"></a></p>
<h3 id="CSP的历史"><a href="#CSP的历史" class="headerlink" title="CSP的历史"></a>CSP的历史</h3><p>1970年，<a href="https://www.computer.org/csdl/trans/tc/1970/04/01671511.pdf" target="_blank" rel="noopener">Fukunaga和Koontz</a>在IEEE Transactions on Computers上发表论文，介绍了一种特征选择的方法，史称“Fukunaga-Koontz变换”，这种特征选择的方法迅速在各个领域得到推广。</p>
<p>1990年，<a href="https://link.springer.com/article/10.1007/BF01129656" target="_blank" rel="noopener">Koles等人</a>将“Fukunaga-Koontz变换”引入背景脑电分析，发现可以通过脑电有效的区分健康人群和精神病人群。 </p>
<p>1999年，<a href="https://www.sciencedirect.com/science/article/pii/S1388245798000388" target="_blank" rel="noopener">MuÈller-Gerking和Pfurtscheller等人</a>（脑-机接口领域有名的Graz研究中心）把Koles的方法（他们对Koles的方法做了一些微小的改进）应用到运动分类上，并称这种方法为“common spatial pattern，CSP”。</p>
<p>次年，<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.7160&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Graz</a>小组的人又把CSP用在运动想象的分类上，取得比较好的分类效果，奠定了CSP在运动想象领域的地位。</p>
<h3 id="CSP（二分类）"><a href="#CSP（二分类）" class="headerlink" title="CSP（二分类）"></a>CSP（二分类）</h3><p>2000年<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.7160&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Graz</a>的论文中提出的CSP是为二分类问题设计的，形式较为简单，然而如果你读CSP相关论文，就会发现CSP存在至少三种表述形式。这三种方式相互联系，又有所区分，很容易让初学者陷入混乱，不知道哪一种是正确形式。</p>
<p>为了解决这一问题，使读者更好的理解其中的数学本质，我接下来从2000年<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.7160&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Graz</a>论文中的算法出发，讨论三种形式间的联系和不同。当然，我假设读者具有线性代数知识（了解特征值分解，了解一些矩阵的操作方法和符号）以及一些基础的numpy或MATLAB数据处理经验，并请读者牢记以下格言</p>
<blockquote><p>DON’T PANIC</p>
<footer><strong>The Hitchhiker's Guide to the Galaxy</strong><cite>Douglas Adams</cite></footer></blockquote>
<h4 id="CSP的原始形式"><a href="#CSP的原始形式" class="headerlink" title="CSP的原始形式"></a>CSP的原始形式</h4><p>假设我们做脑电实验，采集到两类不同的任务信号，用矩阵形式可以表示为$E^{(i)}_1 \in R^{N_c \times S}$和$E^{(i)}_2 \in R^{N_c \times S}$，其中$N_c$表示脑电导联数目，$S$表示采样点的个数，而上标$(i)$表示试次的序号（脑电实验通常会对一种任务进行多次实验，得到很多试次的数据），原始CSP算法采用以下步骤：</p>
<p><strong>第一步</strong>，对数据做decenter处理，即减去每一导联在样本点上的均值</p>
<script type="math/tex; mode=display">
E^{(i)}_1 = E^{(i)}_1 - mean(E^{(i)}_1, axis=1) \\
E^{(i)}_2 = E^{(i)}_2 - mean(E^{(i)}_2, axis=1)</script><p><strong>第二步</strong>，求每一试次的协方差矩阵并归一化，最后得到平均的协方差矩阵</p>
<script type="math/tex; mode=display">
\bar{C}_1 = \sum_{i} \frac{E^{(i)}_1(E^{(i)}_1)^{T}}{trace(E^{(i)}_1(E^{(i)}_1)^{T})} \\
\bar{C}_2 = \sum_{i} \frac{E^{(i)}_2(E^{(i)}_2)^{T}}{trace(E^{(i)}_2(E^{(i)}_2)^{T})}</script><p>这里的函数$trace()$是求矩阵的迹，即主对角线上的元素之和。注意到对于去均值的矩阵$E$，其协方差矩阵可以表示为$C = \frac{1}{S-1} EE^{T}$，上式中没有$S-1$出现是因为上下相除互相抵消。$\bar{C}_1$和$\bar{C}_2$则是两类任务信号的平均协方差矩阵。</p>
<blockquote><p>为什么要使用$trace()$来对协方差矩阵归一化？</p>
<p>1990年Koles的文章中指出，归一化的目的是为了消除”被试间脑电信号幅值的变化”，注意到Koles的原意是区分健康人群和精神疾病人群，而个体的脑电幅值是有绝对性的差异的。方差可以表征信号在时域上的能量高低，不同人群的协方差矩阵的绝对值不同。为了消除这种差异带来的影响，利用$trace()$函数求得所有导联的总体能量，并对协方差矩阵归一化，从而安排除不同个体带来的干扰。Graz小组对同一个体不同试次的数据沿用了这种归一化方式，试图消除试次间的差异，发现也有一定的作用，这种归一化方式就一直流传下来。</p>
<p>然而，有些分析显示这种归一化方式反而会削弱信号的可分性，建议不要使用归一化。关于这一点，我会在以后进行讨论。</p>
</blockquote>
<p><strong>第三步</strong>， 构建复合协方差矩阵，并特征值分解，构建白化（whitening）矩阵</p>
<script type="math/tex; mode=display">
C_c = \bar{C}_1 + \bar{C}_2 \\

C_c = V_cD_cV_c^{T}
=
\begin{bmatrix}
v_1^{(c)} &v_2^{(c)} &\cdots &v_{N_c}^{(c)}
\end{bmatrix}

\begin{bmatrix}
\lambda_1^{(c)} & & &\\
& \lambda_2^{(c)} & &\\
& & \ddots &\\
& & & \lambda_{N_c}^{(c)}
\end{bmatrix} 

\begin{bmatrix}
v_1^{(c)} \\
v_2^{(c)} \\
\vdots \\
v_{N_c}^{(c)}
\end{bmatrix}
\\

P = D_c^{-1/2}V_c^{T}</script><p>其中$V_c$是特征向量矩阵（每一列是特征向量），$D_c$是由特征值组成的对角矩阵。$P$是白化矩阵，其目的是把对角矩阵$D_c$变化为单位矩阵$I$,即$PC_cP^{T} = I$成立。</p>
<p><strong>第四步</strong>，计算空间滤波器$W$</p>
<script type="math/tex; mode=display">
\begin{align}
I = PC_cP^{T} &= P\bar{C}_1P^{T} + P\bar{C}_2P^{T} \\
&= S_1 + S_2 \\
\end{align} \\

S_1 = VD_1V^{T} 
= 
\begin{bmatrix}
v_1 &v_2 &\cdots &v_{N_c}
\end{bmatrix}

\begin{bmatrix}
\lambda_1^{(1)} & & &\\
& \lambda_2^{(1)} & &\\
& & \ddots &\\
& & & \lambda_{N_c}^{(1)} \\
\end{bmatrix} 

\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_{N_c} \\
\end{bmatrix}\\

S_2 = VD_2V^{T} 
= 
\begin{bmatrix}
v_1 &v_2 &\cdots &v_{N_c}
\end{bmatrix}

\begin{bmatrix}
\lambda_1^{(2)} & & &\\
& \lambda_2^{(2)} & &\\
& & \ddots &\\
& & & \lambda_{N_c}^{(2)} \\
\end{bmatrix} 

\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_{N_c} \\
\end{bmatrix}\\

W = P^TV</script><p>其中矩阵$S_1$和$S_2$具有同样的特征向量$V$（这也是共空间模式名称的由来），而相对应的特征值相加始终为1，即$D_1 + D_2 = I$。</p>
<blockquote><p>为什么$S_1$和$S_2$具有同样的特征向量和此消彼长的特征值关系？<br>这一点可以简单的证明如下<br>假设$v_j$和$\lambda_j^{(1)}$分别是$S_1$的特征向量和特征值，即</p>
<script type="math/tex; mode=display">
S_1v_j=\lambda_j^{(1)}v_j</script><p>注意到$S_1+S_2=I$，把上式中的$S_1$置换掉可得</p>
<script type="math/tex; mode=display">
(I-S_2)v_j=\lambda_j^{(1)}v_j</script><p>把上式变形一下可得</p>
<script type="math/tex; mode=display">
S_2v_j=(1-\lambda_j^{(1)})v_j</script><p>显然$v_j$也是$S_2$的特征向量，只不过其特征值为$1-\lambda_j^{(1)}$</p>
</blockquote>
<p>这里还有一点需要注意，我们假设$S_1$特征值的顺序是按降序排列的（那么$S_2$的特征值就是按升序排列），即</p>
<script type="math/tex; mode=display">
1 \ge \lambda_1^{(1)} \ge \lambda_2^{(1)} \ge \dots \ge \lambda_{N_c}^{(1)} \ge 0\\
0 \le \lambda_1^{(2)} \le \lambda_2^{(2)} \le \dots \le \lambda_{N_c}^{(2)} \le 1\\</script><p>这种排序的主要目的是为了以后分析的便利性，例如在运动想象分类中提取最有效的空间滤波器。<br><blockquote><p>协方差矩阵是半正定矩阵（positive semidefinite），而半正定矩阵的特征值均为非负。故$S_1$和$S_2$的特征值在0~1之间</p>
</blockquote></p>
<p>以上就是原始CSP算法的基本内容，在得到空间滤波器矩阵$W$后（$W$的每一列都是一个空间滤波器），就可以对信号进行变换$Z=W^TE$，而$Z$的每一行则代表了滤波后的一个时序特征信号，接下来便可以对$Z$做进一步的分析。</p>
<p>简单回顾一下CSP算法，不难发现CSP实质求解的是这样一个问题，寻找正交矩阵$W$使得以下条件成立：</p>
<script type="math/tex; mode=display">
W^T\bar{C}_1W= D_1 \\
W^T\bar{C}_2W= D_2 \\
D_1 + D_2 = I \\</script><p>让我们对以上的公式做一些变换，把第一个和第二个公式相加</p>
<script type="math/tex; mode=display">
W^T(\bar{C}_1+\bar{C}_2)W=D_1+D_2=I</script><p>又因为$W$是正交矩阵，故$W^T=W^{-1}$，从而</p>
<script type="math/tex; mode=display">
(\bar{C}_1+\bar{C}_2)W=W</script><p>把上式代入$\bar{C}_1W=WD_1$，可得</p>
<script type="math/tex; mode=display">
\bar{C}_1W=(\bar{C}_1+\bar{C}_2)WD_1</script><p>这个式子是不是看起来很像特征向量定义的公式$\bar{C}_1W=WD_1$呢？只不过等式右边多了一个矩阵$\bar{C}_1+\bar{C}_2$。</p>
<p>这类形式的问题叫<a href="http://fourier.eng.hmc.edu/e161/lectures/algebra/node7.html" target="_blank" rel="noopener">广义特征值问题</a>，求解广义特征值问题是脑-机接口领域传统空间滤波方法的基础，大量的算法（CSP、TRCA等）都可以转化为这一形式。</p>
<p>下一小节中，我将从$\bar{C}_1W=(\bar{C}_1+\bar{C}_2)WD_1$这一公式出发来探讨CSP的第二种表述形式。</p>
<h4 id="CSP的第二种表述"><a href="#CSP的第二种表述" class="headerlink" title="CSP的第二种表述"></a>CSP的第二种表述</h4><p>在讨论CSP的第二种表述之前，我们需要了解一个数学概念<a href="https://en.wikipedia.org/wiki/Rayleigh_quotient" target="_blank" rel="noopener">广义雷利商（generalized Rayleigh quotient）</a>。</p>
<p>广义雷利商长这样</p>
<script type="math/tex; mode=display">
R=\frac{w^TAw}{w^TBw} \\
A, B \succeq 0 \\</script><p>其中$A$和$B$为半正定矩阵（读者可以简单理解为协方差矩阵），$w$是列向量，显然广义雷利商$R$是一个实数。</p>
<p>如果我们求如下广义雷利商的优化问题，就会有一些有趣的结果</p>
<script type="math/tex; mode=display">
\underset{w}{\mathrm{max}} \frac{w^TAw}{w^TBw}</script><p>寻找$w$使得$R$最大，在数学上可以等价为求解下式（我就不证明了，感兴趣的读者可点击广义雷利商的链接查看证明过程）</p>
<script type="math/tex; mode=display">
Aw=Bw\lambda_1</script><p>这个公式就是上一节提到的广义特征值问题，也就是说，寻找$w$使广义雷利商最大可以等价为求解$A$和$B$的广义特征值问题并找到使特征值$\lambda_1$最大所对应的特征向量$w$。</p>
<p>如果我们继续寻找能够使$R$第二大、第三大的$w$，就会发现只要解出广义特征值问题的矩阵形式即可</p>
<script type="math/tex; mode=display">
AW= BWD
= B 
\begin{bmatrix}
w_1 &w_2 &\cdots &w_N
\end{bmatrix}

\begin{bmatrix}
\lambda_1 & & &\\
& \lambda_2 & &\\
& & \ddots &\\
& & & \lambda_N \\
\end{bmatrix}</script><p>其中$\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_N$, 特征值的数值也就是广义雷利商的数值。</p>
<p>如果读者明白广义雷利商和广义特征值问题之间的关联，就不难发现，上一节中推导的CSP求解问题可以变形为求解广义雷利商问题</p>
<script type="math/tex; mode=display">
\bar{C}_1W=(\bar{C}_1+\bar{C}_2)WD_1 \ \ \Longleftrightarrow \ \ \underset{w}{\mathrm{max}} \frac{w^T\bar{C}_1w}{w^T(\bar{C}_1+\bar{C}_2)w}</script><p>这里$A=\bar{C}_1$、$B=\bar{C}_1+\bar{C}_2$，$W$矩阵是广义雷利商第一大、第二大至第N大向量$w$组成的集合。</p>
<p>这里我们推出了CSP问题的第二种表述形式，即</p>
<script type="math/tex; mode=display">
\underset{w}{\mathrm{max}} \frac{w^T\bar{C}_1w}{w^T(\bar{C}_1+\bar{C}_2)w}</script><h4 id="CSP的第三种表述"><a href="#CSP的第三种表述" class="headerlink" title="CSP的第三种表述"></a>CSP的第三种表述</h4><p>CSP的第三种表述形式需要绕点弯路。首先还是从CSP的原始形式出发，即寻找正交矩阵$W$使得以下条件成立：</p>
<script type="math/tex; mode=display">
W^T\bar{C}_1W= D_1 \\
W^T\bar{C}_2W= D_2 \\
D_1 + D_2 = I \\</script><p>在第二个公式的左右两边同时右乘矩阵$W^{-1}\bar{C}_2^{-1}$，可以得到</p>
<script type="math/tex; mode=display">
W^T=D_2W^{-1}\bar{C}_2^{-1}</script><p>把该式代入$W^T\bar{C}_1W= D_1$，替换掉$W^T$，可得</p>
<script type="math/tex; mode=display">
D_2W^{-1}\bar{C}_2^{-1}\bar{C}_1W= D_1</script><p>上式左右两边左乘$\bar{C}_2WD_2^{-1}$，可得</p>
<script type="math/tex; mode=display">
\begin{align}
\bar{C}_1W &= \bar{C}_2WD_2^{-1}D_1 \\
&=\bar{C}_2W\Lambda \\
\end{align} \\
\Lambda = D_2^{-1}D_1</script><p>没错，我们又推出了熟悉的广义特征值问题$\bar{C}_1W=\bar{C}_2W\Lambda$，再考虑广义雷利商与之的联系，可以得到CSP的第三种表述</p>
<script type="math/tex; mode=display">
\underset{w}{\mathrm{max}} \frac{w^T\bar{C}_1w}{w^T\bar{C}_2w}</script><blockquote><p>相比CSP的原始形式和第二种表述形式，第三种表述形式更适合从直观上解释CSP在运动想象上有效的原因。<br>运动想象会产生事件相关同步（ERS）和事件相关去同步（ERD）的现象，简单来说就是从电信号上看，某些脑区能量升高，某些脑区能量降低，故能量变化才是运动想象分类的关键特征。</p>
<p>我们前面提高过，方差可以看作一导信号能量的高低（协方差矩阵则是多导信号的能量反应），因此CSP的第三种表述形式实质体现这样一个问题：<br>寻找一种变换方式$w$，使得变换后任务1的能量（$w^T\bar{C}_1w$）和任务2的能量（$w^T\bar{C}_2w$）差异最大化（其比值最大）。</p>
<p>CSP的这种特性恰好和运动想象的现象一致，CSP对能量特征做转换，强化了不同任务间能量差异。</p>
</blockquote>
<p>关于CSP的第三种表述，最后还需要注意的一点是其同CSP原始形式和第二种表述形式并不完全等价，我们在推导第三种表述形式过程种始终没有用到这样一个约束条件$D_1 + D_2 = I$。</p>
<p>这表明，第三种形式是CSP的一种泛化形式，其和CSP原始形式和第二种表述的差异仅在于特征值$\Lambda$不要求在0~1的范围内，具体来说，它们的特征值间存在这样一种关系</p>
<script type="math/tex; mode=display">
\Lambda = D_2^{-1}D_1 \\
D_1 = \Lambda(\Lambda + I)^{-1} \\
D_2 = (\Lambda + I)^{-1} \\</script><h4 id="Talk-is-cheap-Show-me-the-code"><a href="#Talk-is-cheap-Show-me-the-code" class="headerlink" title="Talk is cheap. Show me the code."></a>Talk is cheap. Show me the code.</h4><p>TL;DR<br>我的实践经验表明，CSP第二种和第三种形式总要优于原始形式，主要是能够避免很多数值精度产生的问题。</p>
<p>如果你想试试原生的CSP，选第二种形式。</p>
<p>如果你能够理解其本质，第三种形式或许更合适(我喜欢第三种形式，因为从中可以引出CSP和Riemannian Geometry的关系，这一点以后再谈)</p>
<p>以下是三种CSP的代码<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.linalg <span class="keyword">as</span> linalg</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate data</span></span><br><span class="line">X1 = np.random.rand(<span class="number">20</span>, <span class="number">60</span>, <span class="number">1000</span>)</span><br><span class="line">X2 = np.random.rand(<span class="number">20</span>, <span class="number">60</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">csp1</span><span class="params">(X1, X2)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The first form of CSP.</span></span><br><span class="line"><span class="string">    X1: # of trials, # of channels, # of samples</span></span><br><span class="line"><span class="string">    X2: # of trials, # of channels, # of samples</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Return</span></span><br><span class="line"><span class="string">    W: spatial fitlers</span></span><br><span class="line"><span class="string">    D1: eigenvalues of filters</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    X1 = X1 - X1.mean(axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    X2 = X2 - X2.mean(axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># normalization covariance</span></span><br><span class="line">    C1 = []</span><br><span class="line">    C2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X1.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = X1[i,:,:].dot(X1[i,:,:].T)</span><br><span class="line">        tmp = tmp/np.trace(tmp)</span><br><span class="line">        C1.append(tmp)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X2.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = X2[i,:,:].dot(X2[i,:,:].T)</span><br><span class="line">        tmp = tmp/np.trace(tmp)</span><br><span class="line">        C2.append(tmp)</span><br><span class="line">    C1 = np.array(C1)</span><br><span class="line">    C2 = np.array(C2)</span><br><span class="line">    <span class="comment"># average covariance</span></span><br><span class="line">    mean_C1 = C1.mean(axis=<span class="number">0</span>)</span><br><span class="line">    mean_C2 = C2.mean(axis=<span class="number">0</span>)</span><br><span class="line">    mean_C = mean_C1 + mean_C2</span><br><span class="line">    <span class="comment"># whitening matrix</span></span><br><span class="line">    D_c, V_c = linalg.eigh(mean_C)</span><br><span class="line">    isqrt_D_c = np.diag(np.sqrt(<span class="number">1</span>/D_c))</span><br><span class="line">    P = isqrt_D_c@V_c.T</span><br><span class="line">    <span class="comment"># S1 and S2</span></span><br><span class="line">    S1 = P@mean_C1@P.T</span><br><span class="line">    S2 = P@mean_C2@P.T</span><br><span class="line">    <span class="comment"># spatial filters</span></span><br><span class="line">    D1, V = linalg.eigh(S1)</span><br><span class="line">    W = P.T@V</span><br><span class="line">    <span class="keyword">return</span> W, D1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">csp2</span><span class="params">(X1, X2)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The second form of CSP.</span></span><br><span class="line"><span class="string">    X1: # of trials, # of channels, # of samples</span></span><br><span class="line"><span class="string">    X2: # of trials, # of channels, # of samples</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Return</span></span><br><span class="line"><span class="string">    W: spatial fitlers</span></span><br><span class="line"><span class="string">    D1: eigenvalues of filters</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    X1 = X1 - X1.mean(axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    X2 = X2 - X2.mean(axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># normalization covariance</span></span><br><span class="line">    C1 = []</span><br><span class="line">    C2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X1.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = X1[i,:,:].dot(X1[i,:,:].T)</span><br><span class="line">        tmp = tmp/np.trace(tmp)</span><br><span class="line">        C1.append(tmp)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X2.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = X2[i,:,:].dot(X2[i,:,:].T)</span><br><span class="line">        tmp = tmp/np.trace(tmp)</span><br><span class="line">        C2.append(tmp)</span><br><span class="line">    C1 = np.array(C1)</span><br><span class="line">    C2 = np.array(C2)</span><br><span class="line">    <span class="comment"># average covariance</span></span><br><span class="line">    mean_C1 = C1.mean(axis=<span class="number">0</span>)</span><br><span class="line">    mean_C2 = C2.mean(axis=<span class="number">0</span>)</span><br><span class="line">    mean_C = mean_C1 + mean_C2</span><br><span class="line">    <span class="comment"># generalized eigenvalue problem</span></span><br><span class="line">    D1, W = linalg.eigh(mean_C1,mean_C)</span><br><span class="line">    <span class="keyword">return</span> W, D1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">csp3</span><span class="params">(X1, X2)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The third form of CSP</span></span><br><span class="line"><span class="string">    X1: # of trials, # of channels, # of samples</span></span><br><span class="line"><span class="string">    X2: # of trials, # of channels, # of samples</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Return</span></span><br><span class="line"><span class="string">    W: spatial fitlers</span></span><br><span class="line"><span class="string">    D: eigenvalues of filters</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    X1 = X1 - X1.mean(axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    X2 = X2 - X2.mean(axis=<span class="number">2</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># normalization covariance</span></span><br><span class="line">    C1 = []</span><br><span class="line">    C2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X1.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = X1[i,:,:].dot(X1[i,:,:].T)</span><br><span class="line">        tmp = tmp/np.trace(tmp)</span><br><span class="line">        C1.append(tmp)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X2.shape[<span class="number">0</span>]):</span><br><span class="line">        tmp = X2[i,:,:].dot(X2[i,:,:].T)</span><br><span class="line">        tmp = tmp/np.trace(tmp)</span><br><span class="line">        C2.append(tmp)</span><br><span class="line">    C1 = np.array(C1)</span><br><span class="line">    C2 = np.array(C2)</span><br><span class="line">    <span class="comment"># average covariance</span></span><br><span class="line">    mean_C1 = C1.mean(axis=<span class="number">0</span>)</span><br><span class="line">    mean_C2 = C2.mean(axis=<span class="number">0</span>)</span><br><span class="line">    D, W = linalg.eigh(mean_C1, mean_C2)</span><br><span class="line">    <span class="keyword">return</span> W, D</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W1, D1 = csp1(X1, X2)</span><br><span class="line">W2, D2 = csp2(X1, X2)</span><br><span class="line">W3, D3 = csp3(X1, X2)</span><br></pre></td></tr></table></figure>
<p>让我们看一下CSP原始形式的第一个空间滤波器的特征值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D1[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>0.4601822497625514
</code></pre><p>再看一下CSP第二种表述形式的第一个空间滤波器的特征值，两者大致相同，但最后几位不太一样，这是由于数值舍入精度的问题（或许跟如何实施算法有关），关于这方面的问题我也在研究中，目前就忽略吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D2[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>0.46018224976254957
</code></pre><p>再看一下CSP第三种表述形式的第一个空间滤波器的特征值，好像跟上面两者差别很大呢。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D3[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>0.8524770620457159
</code></pre><p>回顾一下前文讨论的特征值间的联系，简单的做个变换，发现我们的算法是正确的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># connection between form2 and form3</span></span><br><span class="line">new_D = D3/(D3+<span class="number">1</span>)</span><br><span class="line">new_D[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>0.4601822497625497
</code></pre><p>最后来看下空间滤波器（特征向量）是否正确，我们来检验第一特征向量的前5个数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W1[:<span class="number">5</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])
</code></pre><p>第二种形式和CSP原始形式完全一致（有时会差一个正负号，但从算法上来说是完全正确的）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># maybe negative, but that's ok</span></span><br><span class="line">W2[:<span class="number">5</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])
</code></pre><p>看一下第三种形式。咦，好像不太对哦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># doesn't look like the result above. Is there something wrong?</span></span><br><span class="line">W3[:<span class="number">5</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([ 2.13472472,  0.9693093 , -0.69313345,  0.71057786, -0.40544634])
</code></pre><p>别慌，第三种形式只和第一种和第二种差了一个倍数（向量是一样的），做如下变换就能得到一样的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Nothing is wrong here.</span></span><br><span class="line">np.sqrt(<span class="number">1</span>/(D3+<span class="number">1</span>))[<span class="number">0</span>]*W3[:<span class="number">5</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([ 1.56843115,  0.71217374, -0.50926103,  0.52207783, -0.29789071])
</code></pre><h3 id="CSP（多分类）"><a href="#CSP（多分类）" class="headerlink" title="CSP（多分类）"></a>CSP（多分类）</h3>
            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">标签</span><br>
                
    <a class="tag tag--primary tag--small t-link" href="/tags/brain-computer-interface/">brain-computer interface</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/zh-cn/2019/01/08/（六）Psychopy事件响应/" data-tooltip="（六）Psychopy事件响应" aria-label="上一篇: （六）Psychopy事件响应">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/zh-cn/2018/11/16/（五）如何在Psychopy中新建窗口/" data-tooltip="（五）如何在Psychopy中新建窗口" aria-label="下一篇: （五）如何在Psychopy中新建窗口">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Diesen Beitrag teilen">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2019 swolf. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/zh-cn/2019/01/08/（六）Psychopy事件响应/" data-tooltip="（六）Psychopy事件响应" aria-label="上一篇: （六）Psychopy事件响应">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/zh-cn/2018/11/16/（五）如何在Psychopy中新建窗口/" data-tooltip="（五）如何在Psychopy中新建窗口" aria-label="下一篇: （五）如何在Psychopy中新建窗口">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Diesen Beitrag teilen">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#table-of-contents" aria-label="目录">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="5">
    <i id="btn-close-shareoptions" class="fa fa-times"></i>
    <ul class="share-options">
        
    </ul>
</div>

            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/avatar0.jpg" alt="作者的图片">
        
            <h4 id="about-card-name">swolf</h4>
        
            <div id="about-card-bio"><p>生物医学工程在读博士，主要研究方向为脑机接口、机器学习和神经科学理论</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br>
                <p>目前失业中…</p>

            </div>
        
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cyberpunk.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-vufjrm3fmbuttogo1hxuu0w9w0sesk5iyysjuguc2hdhufot9szxg8twijry.min.js"></script>
<!--SCRIPTS END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    



    </body>
</html>
